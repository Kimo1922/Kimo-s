# -*- coding: utf-8 -*-
"""Copy of naiveBitcoin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OCg8dYuCSbdLuYTDSfNHzl-rKzhNyBY2

# Setup

### Install modules as needed
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !python -m pip install umap-learn
# # capture suppresses output. remove to troubleshoot umap installation

"""### Load modules"""

import re
import umap
import nltk
import random
import sklearn
import datetime

import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt

"""#### Finish setting up modules"""

# download punk for tokenizing
nltk.download("punkt")
nltk.download("vader_lexicon")

"""### Load data into pandas dataframes"""

# read data into a Pandas dataframe. make sure you upload files.
prices = pd.read_csv("BTC-USD (1).csv")
tweets = pd.read_csv("small_dataset (1).csv")
print(tweets)

"""# Preprocess Data
The price data is bi-daily, with prices given at the opening of trading (9:30 a.m.) and close (4:00 p.m.). Tweets have a time resolution up to a second. We will upsample the price data to being hourly by using interpolation and aggregate tweets so both can be on an hourly time scale.

## Upsample prices and generate labeled data
Convert open and close data to an interpolated time series.

### Generate price time series
"""

# convert string to Python date object
prices["Open Date" ] = pd.to_datetime(prices["Date"]) + datetime.timedelta(hours = 9, minutes=30)
prices["Close Date"] = pd.to_datetime(prices["Date"]) + datetime.timedelta(hours = 16)

#make two pandas Series, indexed by time
idx              = pd.DatetimeIndex(prices["Open Date"])
dat              = prices["Open"]
openSeries       = pd.Series(data=dat)
openSeries.index = idx

idx                = pd.DatetimeIndex(prices["Close Date"])
dat                = prices["Close"]
closedSeries       = pd.Series(data=dat)
closedSeries.index = idx

series = pd.concat([openSeries,closedSeries])
series.sort_index(inplace=True)
# will plot prices multiple times so putting this code in a method for uniform style in plots
def plotSeries(series,dotSize=2):
    plt.clf()
    plt.figure(dpi=400)
    plt.scatter(series.index,series,s=dotSize,color="red")
    plt.plot(series.index,series,color="blue",linewidth=0.5)
plotSeries(series)

"""### Interpolate time series for higher resolution"""

# since open/close is at 9:30/4 p.m. we need to interpolate prices
# but first resample to 1 hour so the intervals are evenly spaced in time
priceSeries  = series.resample("1H").interpolate(method="polynomial",order=5)

print(priceSeries)
plotSeries(priceSeries,dotSize=0.1)

"""### Find speed and acceleration of prices"""

# speed = first  derivative
speed = np.gradient(priceSeries)
# accel = second derivative
accel = np.gradient(speed)

plt.clf()
plt.figure(dpi=800)
plt.scatter(priceSeries.index,speed,s=0.01,color="blue")
plt.scatter(priceSeries.index,accel,s=0.01,color="orange")

# put the price, acceleration, speed, and tweets into a single dataframe indexed by hour
df = pd.DataFrame({"Price": priceSeries, "Speed": speed, "Acceleration": accel})
df.dropna(inplace=True) # remove na values generated from upsampling
# get a visual on the acceleration only
plt.clf()
plt.figure(dpi=800)
plt.hist(df["Acceleration"],bins="sturges",density=True)
df["Acceleration"].plot(kind="density")

"""### Make labels for acceleration"""

# discretize acceleration values into classes; if normal then use z-scores
# shapiro wilk p value is unreliable for N > 5000, take 10 random samples (no replacement) with N = 2500
for i in range(10):
    print("Not normal" if stats.shapiro(random.sample(list(df["Acceleration"]),2500)).pvalue < 0.001 else "Normal") 

acc = df["Acceleration"]
acc = (acc - np.mean(acc)) / np.std(acc) # set mean to 0 and sd to 1; mostly for eyeballing convenience
plt.clf()
plt.figure(dpi=200)
acc.plot(kind="density")
x = np.linspace(min(acc),max(acc),1000)
plt.plot(x, stats.norm.pdf(x, np.mean(acc), np.std(acc)))

"""Anything beyond one standard deviation seems rare."""

levels = [ "UP" if a >=1.0 else "DN" if a <= -1.0 else "ZR" for a in acc]
df["hypotheses"] = levels
print(levels)

# label tweet by hours
tweets["onlyHours"] = pd.to_datetime([ re.sub("(.*T[0-9]{2})(.*)",r"\1",s) for s in tweets["timestamp"] ])
print("before")
print(tweets["timestamp"].tail())
print("after")
print(tweets["onlyHours"].tail())

"""### Combine tweet and price dataframes, join on hour"""

tp = tweets.join(df,on="onlyHours",how="inner") # tp := tweets AND prices
tp.head()

tp = tp.sample(n = 2000,axis="index",random_state=96024) # crashes with full dataset so we take subsample of size 2000
print(tp)

"""# Tweets to Bag of Words

### Visualize the data
Visualization gives an idea of whether or not there does exist a difference in the distributions for each hypothesis.
"""

# use countvectorizer for bag of words embedding
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import StandardScaler

count_vect = CountVectorizer()
bagOfWordsVectors = count_vect.fit_transform(tp["text"])
bagOfWordsVectors = StandardScaler().fit_transform(bagOfWordsVectors.toarray())
print(bagOfWordsVectors.shape)

# use umap for dimensionality reduction
mapper = umap.UMAP(n_components=3,n_neighbors=20,random_state=88)
accelD = {"UP" : 1 , "DN" : -1, "ZR" : 0}
embed = mapper.fit_transform(bagOfWordsVectors, y=[accelD[h]for h in tp["hypotheses"]])
print(embed)

"""#### Add the embeddings to tp"""

tp["embedX"] = embed[:,0]
tp["embedY"] = embed[:,1]
tp["embedZ"] = embed[:,2]
print(tp)

import plotly.express as px
fig = px.scatter_3d(tp,x="embedX",y="embedY",z="embedZ",
                color="hypotheses")
fig.update_traces(marker_size = 5)
fig.show()

"""### Train the Naive Bayes Classifier"""

train, test = sklearn.model_selection.train_test_split(tp)

"""#### **[TO DO]** Write the getFeatures function"""

from nltk.stem.porter import PorterStemmer 
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.tokenize import TweetTokenizer
from math import ceil
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
import re
import string
stemmer = PorterStemmer()
sa = SentimentIntensityAnalyzer()

def getFeatures(tweet,embedX,embedY,embedZ):
    featureDict = {}
    featureDict["feature1"] = tweet.count("pump")
    featureDict["feature2"] = tweet.count("crash")
    return featureDict

train_set  = [ (getFeatures(tweet,embedX,embedY,embedZ),label) for tweet,label,embedX,embedY,embedZ
               in zip(train["text"],train["hypotheses"],train["embedX"],train["embedY"],train["embedZ"]) ]

"""### Train the classifier"""



classifier = nltk.NaiveBayesClassifier.train(train_set)

"""### Test your classifier

#### Make the testing set
"""

test_set  = [ (getFeatures(tweet,embedX,embedY,embedZ),label) for tweet,label,embedX,embedY,embedZ
               in zip(test["text"],test["hypotheses"],test["embedX"],test["embedY"],test["embedZ"]) ]

"""### Evaluate the acccuracy of your classifier and show the most informative features"""

print(nltk.classify.accuracy(classifier, test_set))
classifier.show_most_informative_features(20)

